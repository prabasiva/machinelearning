\documentclass[a4paper,12pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{parskip}
\usepackage{times}

\begin{document}

\section*{Mathematical Description of Logistic Regression}

Logistic regression is a statistical model used for binary classification, extendable to multiclass problems via techniques like softmax regression. It predicts the probability that a given input belongs to a particular class. Below is the mathematical formulation.

\subsection*{1. Model Representation}
For a binary classification problem, the goal is to predict the probability $P(y=1 | \mathbf{x})$, where $y \in \{0, 1\}$ is the class label, and $\mathbf{x} \in \mathbb{R}^n$ is the feature vector. The logistic regression model assumes this probability follows the logistic (sigmoid) function:

\[
P(y=1 | \mathbf{x}) = \sigma(\mathbf{w}^T \mathbf{x} + b) = \frac{1}{1 + e^{-(\mathbf{w}^T \mathbf{x} + b)}}
\]

\begin{itemize}
    \item $\mathbf{w} \in \mathbb{R}^n$: Weight vector (parameters to be learned).
    \item $b \in \mathbb{R}$: Bias term (intercept).
    \item $\mathbf{w}^T \mathbf{x} + b$: Linear combination of features, often denoted as $z$.
    \item $\sigma(z) = \frac{1}{1 + e^{-z}}$: Sigmoid function, mapping $z \in \mathbb{R}$ to $[0, 1]$.
\end{itemize}

The probability of the negative class is:

\[
P(y=0 | \mathbf{x}) = 1 - P(y=1 | \mathbf{x}) = \frac{e^{-(\mathbf{w}^T \mathbf{x} + b)}}{1 + e^{-(\mathbf{w}^T \mathbf{x} + b)}}.
\]

\subsection*{2. Decision Rule}
To classify an input $\mathbf{x}$, a threshold (typically 0.5) is applied to the predicted probability:

\[
\hat{y} =
\begin{cases} 
1 & \text{if } P(y=1 | \mathbf{x}) \geq 0.5 \\
0 & \text{otherwise}
\end{cases}
\]

Since $P(y=1 | \mathbf{x}) = \sigma(z)$, and $\sigma(z) = 0.5$ when $z = 0$, this is equivalent to:

\[
\hat{y} =
\begin{cases} 
1 & \text{if } \mathbf{w}^T \mathbf{x} + b \geq 0 \\
0 & \text{otherwise}
\end{cases}
\]

\subsection*{3. Loss Function}
The parameters $\mathbf{w}$ and $b$ are learned by minimizing the \textbf{log-loss} (or \textbf{binary cross-entropy loss}). For a dataset of $m$ samples $\{ (\mathbf{x}^{(i)}, y^{(i)}) \}_{i=1}^m$, the log-loss is:

\[
J(\mathbf{w}, b) = -\frac{1}{m} \sum_{i=1}^m \left[ y^{(i)} \log(P(y=1 | \mathbf{x}^{(i)})) + (1 - y^{(i)}) \log(1 - P(y=1 | \mathbf{x}^{(i)})) \right]
\]

Substituting $P(y=1 | \mathbf{x}^{(i)}) = \sigma(\mathbf{w}^T \mathbf{x}^{(i)} + b)$, the loss becomes:

\[
J(\mathbf{w}, b) = -\frac{1}{m} \sum_{i=1}^m \left[ y^{(i)} \log(\sigma(\mathbf{w}^T \mathbf{x}^{(i)} + b)) + (1 - y^{(i)}) \log(1 - \sigma(\mathbf{w}^T \mathbf{x}^{(i)} + b)) \right]
\]

\subsection*{4. Optimization}
The loss function $J(\mathbf{w}, b)$ is convex, so optimization techniques like \textbf{gradient descent} are used to find the optimal parameters. The gradients with respect to $\mathbf{w}$ and $b$ are:

\[
\frac{\partial J}{\partial \mathbf{w}} = \frac{1}{m} \sum_{i=1}^m \left( \sigma(\mathbf{w}^T \mathbf{x}^{(i)} + b) - y^{(i)} \right) \mathbf{x}^{(i)}
\]

\[
\frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m \left( \sigma(\mathbf{w}^T \mathbf{x}^{(i)} + b) - y^{(i)} \right)
\]

In gradient descent, the parameters are updated iteratively:

\[
\mathbf{w} \leftarrow \mathbf{w} - \alpha \frac{\partial J}{\partial \mathbf{w}}, \quad b \leftarrow b - \alpha \frac{\partial J}{\partial b}
\]

where $\alpha$ is the learning rate.

\subsection*{5. Regularization (Optional)}
To prevent overfitting, regularization terms (e.g., L2 or L1) can be added to the loss function. For \textbf{L2 regularization}, the loss becomes:

\[
J(\mathbf{w}, b) = -\frac{1}{m} \sum_{i=1}^m \left[ y^{(i)} \log(\sigma(\mathbf{w}^T \mathbf{x}^{(i)} + b)) + (1 - y^{(i)}) \log(1 - \sigma(\mathbf{w}^T \mathbf{x}^{(i)} + b)) \right] + \frac{\lambda}{2} \| \mathbf{w} \|^2
\]

\begin{itemize}
    \item $\lambda$: Regularization parameter controlling the strength of the penalty.
    \item $\| \mathbf{w} \|^2$: L2 norm of the weights (encourages smaller weights).
\end{itemize}

The gradient for $\mathbf{w}$ is modified to include the regularization term:

\[
\frac{\partial J}{\partial \mathbf{w}} = \frac{1}{m} \sum_{i=1}^m \left( \sigma(\mathbf{w}^T \mathbf{x}^{(i)} + b) - y^{(i)} \right) \mathbf{x}^{(i)} + \lambda \mathbf{w}
\]

\subsection*{6. Multiclass Extension (Softmax Regression)}
For $K$-class classification, logistic regression is generalized to \textbf{softmax regression}. The model outputs probabilities for each class using the softmax function:

\[
P(y=k | \mathbf{x}) = \frac{e^{\mathbf{w}_k^T \mathbf{x} + b_k}}{\sum_{j=1}^K e^{\mathbf{w}_j^T \mathbf{x} + b_j}}
\]

\begin{itemize}
    \item $\mathbf{w}_k, b_k$: Parameters for class $k$.
\end{itemize}

The loss function is the \textbf{categorical cross-entropy}, and optimization proceeds similarly.

\subsection*{7. Summary}
Logistic regression models the probability of a binary outcome using the sigmoid function, optimizing parameters via gradient descent on the log-loss. Regularization can be applied to improve generalization. The model is interpretable, computationally efficient, and widely used for binary classification tasks.

\end{document}2swrite mathematical description in latex in english about random forestRandom_Forest.texlatexâ€¢2.7sHow can Grok help?